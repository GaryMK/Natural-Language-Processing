# Natural-Language-Processing
Natural Language Processing project practices.

## 笔记

### 概述

#### 应用

- 拼写检查、关键词检索······
- 文本挖掘（产品价格、日期、时间、地点、人名、公司名）
- 文本分类
- 机器翻译
- 客服系统
- 复杂对话系统（微软小冰）

#### 为什么结合深度学习

- 手工特征耗时耗力，还不易拓展
- 自动特征学习快，方便拓展
- 深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息
- 深度学习既可以无监督学习，也可以有监督学习

### 语言模型

p(S) = p(w1, w2, w3, w4, w5, ..., wn)

​		=p(w1)p(w2|w1)p(w3|w1, w2)...p(wn|w1, w2, ..., wn - 1)

p(S) 被称为语言模型，即用来计算一个句子概率的模型

**问题：**

1. 数据过于稀疏
2. 参数空间太大

**解决方案：**

1. 假设下一个词的出现依赖它前面的一个词（n-gram = 1）

   p(S) = p(w1)p(w2|w1)p(w3|w2)...p(wn|wn-1)

2. 假设下一个词的出现依赖它前面的两个词（n-gram = 2）

   p(S) = p(w1)p(w2|w1)p(w3|w1, w2)...p(wn|wn-1, wn-2)

p(I want chinese food) = P(want|I) * P(chinese|want) * P(food|chinese)

> 奥卡姆剃刀，能用简单模型就用简单模型，防止参数过多导致过拟合
>
> n-gram 参数一般选 2

### Word2Vec

#### 词向量

词向量的编码方式：

- One hot representation：词汇表一般非常大，表达的效率不高
- Distributed representation：通过训练，将模型都映射到一个较短的词向量上。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。词向量维度一般需要训练时自己来指定。**（一般使用神经网络语言模型训练得到合适的词向量）**

词向量的突出特点：

词向量模型中，词向量与词向量之间有这非常特殊的特性。**例如现在存在国王、男生、女人、皇后四个词向量，那么一个完善的词向量模型，就存在“国王-男人+女人=皇后”这样的关系。**

#### 神经网络语言模型

##### CBOW

CBOW是 Continuous Bag-of-Words Model 的缩写，是一种根据上下文的词语预测当前词语的出现概率的模型

**输入层**是上下文的词语的词向量，在训练CBOW模型，词向量只是个**副产品**，确切来说，是CBOW模型的一个参数。训练开始的时候，词向量是个随机值，随着训练的进行不断被更新。由于CBOW使用的是**词袋模型**，因此上下文的多个词都是平等的，也就是**不考虑他们和我们关注的词之间的距离大小**，只要在我们上下文之内即可。

> **词袋模型(BOW,bag of words):**
>
> 将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。
>
> **词向量模型(Word Embedding):**
>
> 词向量模型是考虑词语位置关系的一种模型。
>
> **现在常用word2vec构成词向量模型，它的底层采用基于CBOW和Skip-Gram算法的神经网络模型。**

**投影层**对其求和，所谓求和，就是简单的向量加法。

**输出层(softmax层)**输出最可能的w。由于语料库中词汇量是固定的|C|个，所以上述过程其实可以看做一个多分类问题。给定特征，从|C|个分类中挑一个。

> 哈夫曼树：
>
> 哈夫曼树是一种带权路径长度最短的二叉树，也称为最优二叉树。

##### Skip-Graw

Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。

#### 哈夫曼树

Word2Vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用**霍夫曼树来代替隐藏层和输出层的神经元**，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。

**使用哈夫曼树的好处：**一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望**越常用的词拥有更短的编码。**

#### 参数更新

求最大概率，求解是梯度上升

#### Hierarchica Softmax

#### Negative Sampling

#### 混淆矩阵

#### 词云

#### TF-IDF

### GAN

判别器：分辨出生成的和真实的

生成器：生成可以骗过判别器的

损失函数：一方面要让判别器分辨能力更强，另一方面要让生成器更真

### DCGAN

判别模型：使用带步长的卷积（strided convolutions）取代了的空间池化（spatial pooling），容许网络学习自己的空间下采样（spatial downsampling）

生成模型：使用微步幅卷积（fractional strided），容许它学习自己的空间上采样（spatial unsampling）

激活函数：LeakyReLU

Batch Normalization 批标准化：解决因糟糕的初始化引起的训练问题，使得梯度能传播更深层次。Batch Normalization 证明了生成模型初始化的重要性，避免生成模型崩溃：生成的所有样本都在一个点上（样本相同），这是训练GANs经常遇到的失败现象。

> 梯度消失，梯度爆炸

### RNN

### LSTM

C：控制参数

决定什么样的信息会被保留什么样的会被遗忘

### Style-Transfer

### Seq2Seq：机器翻译

## 项目实战

**[Task1: 影评情感分类]()**

**[Task2: 基于Word2Vec的LSTM模型]()**

**[Task3: 基于BERT的中文情感分析]()**

**[Task4: 基于BERT的中文命名实体识别]()**

**[Task5: RNN网络架构与情感分析应用]()**

**[Task6: 医学糖尿病数据命名实体识别]()**